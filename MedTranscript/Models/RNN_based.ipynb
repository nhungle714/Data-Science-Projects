{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - RNN Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nhungle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = pkl.load(open(os.path.join(\"../data\",\n",
    "                                          \"train_data_tokens.p\"), \"rb\"))\n",
    "\n",
    "val_tokens = pkl.load(open(os.path.join(\"../data\",\n",
    "                                          \"val_data_tokens.p\"), \"rb\"))\n",
    "\n",
    "test_tokens = pkl.load(open(os.path.join(\"../data\",\n",
    "                                          \"test_data_tokens.p\"), \"rb\"))\n",
    "\n",
    "all_tokens = pkl.load(open(os.path.join(\"../data\",\n",
    "                                          \"all_data_tokens.p\"), \"rb\"))\n",
    "\n",
    "train_target = pkl.load(open(os.path.join(\"../data\", \"train_target.p\"), \"rb\"))\n",
    "val_target = pkl.load(open(os.path.join(\"../data\", \"val_target.p\"), \"rb\"))\n",
    "test_target = pkl.load(open(os.path.join(\"../data\", \"test_target.p\"), \"rb\"))\n",
    "label_mapping = pkl.load(open(os.path.join(\"../data\", \"target_mapping.p\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self, all_tokens, max_vocab_size, PAD_IDX, UNK_IDX):\n",
    "        res = self.buildVocab(all_tokens, max_vocab_size, PAD_IDX, UNK_IDX)\n",
    "        # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "        self.id2token = res[1]\n",
    "        # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "        self.token2id = res[0]\n",
    "    \n",
    "    def buildVocab(self, all_tokens, max_vocab_size, PAD_IDX, UNK_IDX):\n",
    "        token_counter = Counter(all_tokens)\n",
    "        vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "        id2token = list(vocab)\n",
    "        token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "        id2token = ['<pad>', '<unk>'] + id2token\n",
    "        token2id['<pad>'] = PAD_IDX \n",
    "        token2id['<unk>'] = UNK_IDX\n",
    "        return token2id, id2token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.id2token)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 30000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "corpus = Dictionary(all_tokens, max_vocab_size, PAD_IDX, UNK_IDX)\n",
    "id2token = corpus.id2token\n",
    "token2id = corpus.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30002"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert token (word) to ids\n",
    "\n",
    "For each dataset, each sample of tokens (i.e., words) will be represented as index of that word in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token2IndexDataset(object):\n",
    "    def __init__(self, tokens_data):\n",
    "        self.indices_data = self.token2index_dataset(tokens_data)\n",
    "        \n",
    "    def token2index_dataset(self, tokens_data):\n",
    "        indices_data = []\n",
    "        for tokens in tokens_data:\n",
    "            index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "            indices_data.append(index_list)\n",
    "        return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_indices = Token2IndexDataset(train_tokens).indices_data\n",
    "val_data_indices= Token2IndexDataset(val_tokens).indices_data\n",
    "test_data_indices= Token2IndexDataset(test_tokens).indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 15607 ; token crosslink\n",
      "Token crosslink; token id 15607\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "import random\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using deep learning methods on NLP tasks, we usually utilize [word embedding](https://en.wikipedia.org/wiki/Word_embedding). To put it briefly, word embedding represent words, or tokens, in a vocabulary as a distributed numerical vector. There are a lot of methods to obtain a word embedding, with some of the most famous shallow models being Word2Vec, GloVe, and FastText while the deeper models are BERT, RoBERTa, T5. It is not difficult to find a general purpose word embedding trained by one of the aforementioned methods on the Internet that's been trained with a massive amount of data. It is usually a good idea to use these pre-trained embedding to save yourself some time and computing resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = glove2word2vec(os.path.join(\"../data\",'glove.6B.50d.txt'), 'tmp_file')\n",
    "glove_embedding = KeyedVectors.load_word2vec_format('tmp_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check the dimension is 50\n",
    "len(glove_embedding['brain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find similar words\n",
    "\n",
    "The word embedding vectors can help us find words with similar meanings. Word similarities can be measured by [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). The function below looks up the most similar words to a given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('underwent', 0.8644347190856934),\n",
       " ('arthroscopic', 0.8504809141159058),\n",
       " ('undergoing', 0.8430145382881165),\n",
       " ('reconstructive', 0.8339141607284546),\n",
       " ('surgeries', 0.8272889852523804)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding.similar_by_word('surgery', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token2VectorDataset(object):\n",
    "    def __init__(self, tokens_data, embedding,):\n",
    "        self.tokens_data = tokens_data\n",
    "        self.embedding = embedding\n",
    "        self.UNK_IDX = UNK_IDX\n",
    "        self.indices_data = self.token2vector_dataset()\n",
    "        \n",
    "    def token2vector_dataset(self):\n",
    "        indices_data = []\n",
    "        for tokens in self.tokens_data:\n",
    "            index_list = [self.embedding[token] if token in self.embedding.vocab else UNK_IDX\n",
    "                          for token in tokens]\n",
    "            indices_data.append(index_list)\n",
    "        return indices_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vectors = Token2VectorDataset(train_tokens, glove_embedding).indices_data\n",
    "val_data_vectors = Token2VectorDataset(val_tokens, glove_embedding).indices_data\n",
    "test_data_vectors = Token2VectorDataset(test_tokens, glove_embedding).indices_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between train_data_indices and train_data_vectors\n",
    "\n",
    "Both train_data_indices and train_data_vectors have 1571 data points, each represent a sentence (or a text data)\n",
    "\n",
    "However, for train_data_indices, each sample is a list of token, each token represents a word in the corpus.\n",
    "\n",
    "For train_data_vectors, each sample is a list of arrays, each array is 50-dimension vector that represents the distance of that word in the embedding space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MedTranscriptDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide MAX_SENTENCE_LENGTh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473.9898154042011"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(i) for i in train_data_indices]\n",
    "np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medtranscript_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "#     batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    #print(data_list)\n",
    "    #print(length_list)\n",
    "    #print(label_list)\n",
    "    return [torch.from_numpy(np.array(data_list)),\n",
    "            torch.LongTensor(length_list),\n",
    "            torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = MedTranscriptDataset(train_data_indices, train_target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=medtranscript_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = MedTranscriptDataset(val_data_indices, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=medtranscript_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MedTranscriptDataset(test_data_indices, test_target)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=medtranscript_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   94,    70,  1720,  ...,    12,   795,   784],\n",
      "        [   39, 26780,   725,  ...,     0,     0,     0],\n",
      "        [   94,    70, 11367,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   94,    70,    21,  ...,   212,   870,    12],\n",
      "        [ 2001,     5,    39,  ...,     0,     0,     0],\n",
      "        [   39,   651,    18,  ...,    52,     3,    28]])\n",
      "tensor([4, 4, 4, 4, 4, 2, 0, 0, 4, 4, 1, 0, 4, 0, 1, 1, 4, 4, 3, 2, 0, 3, 3, 0,\n",
      "        1, 3, 4, 1, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "data, lengths, labels = next(iter(test_loader))\n",
    "print(data)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, we will be exploring two variants of RNN: vanilla (or Elman) RNN and LSTM (Long-short term memory).\n",
    "\n",
    "- Each input word is represented by a vector of dimension ```embedding_dim```. Check out ```nn.Embedding``` to see how to initialize embeddings randomly.\n",
    "- Your model should take the following input parameters\n",
    "    - ```hidden_dim```: The number of features in the hidden state h of your RNN layer\n",
    "    - ```output_dim```: Number of output classes\n",
    "    - ```vocab_size``` Size of your vocabulary. \n",
    "    - ```embedding_dim```: Dimension of word embeddings\n",
    "- Your model should consist of an RNN layer (you can use either ```nn.RNN``` or ```nn.LSTM```) followed by a linear layer.\n",
    "- $h_{0}$ (and $c$ if you use LSTM) should be initialized as a zero vector of dimension ```hidden_dim```. You might want to check out ```nn.Parameter```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating three layers: \n",
    "- Embedding\n",
    "- RNN\n",
    "- Fully Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_len, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 600])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim=40\n",
    "output_dim=5\n",
    "vocab_size=len(corpus)\n",
    "embedding_dim=50\n",
    "rnn='RNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=PAD_IDX)\n",
    "rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "fc = nn.Linear(hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_x = emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 600])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 600, 50])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emb_x: [batch_size, len_x, embedding_dim]\n",
    "emb_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden = rnn(emb_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 600, 40])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to reshape before passing through fully connected layer\n",
    "output = output.reshape(-1, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19200, 40])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19200, 5])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=50\n",
    "hidden_dim=40\n",
    "output_dim=5\n",
    "num_layers=1\n",
    "rnn_dropout= 0.1\n",
    "options = {\n",
    "            'num_embeddings': len(corpus),\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'padding_idx': PAD_IDX,\n",
    "            'input_size': embedding_dim,\n",
    "            'hidden_size': hidden_dim,\n",
    "            'num_layers': num_layers,\n",
    "            'rnn_dropout': rnn_dropout,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This model combines embedding, rnn and projection layer into a single model\n",
    "    \"\"\"\n",
    "    def __init__(self, options):\n",
    "        #optioins: a dictionary with key = argument, value = value of that argument\n",
    "        super().__init__()\n",
    "        \n",
    "        # create each LM part here \n",
    "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], \n",
    "                                   embedding_dim=options['embedding_dim'], \n",
    "                                   padding_idx=options['padding_idx'])\n",
    "        self.rnn = nn.RNN(options['input_size'],\n",
    "                          options['hidden_size'],\n",
    "                          options['num_layers'],\n",
    "                          dropout=options['rnn_dropout'],\n",
    "                          batch_first=True)\n",
    "        self.projection = nn.Linear(options['hidden_size'],\n",
    "                                    options['num_embeddings'])\n",
    "        \n",
    "    def forward(self, encoded_input_sequence):\n",
    "        \"\"\"\n",
    "        Forward method process the input from token ids to logits\n",
    "        \"\"\"\n",
    "        embeddings = self.lookup(encoded_input_sequence) #rnn gives u all outputs / hidden as it has so far.\n",
    "        rnn_outputs = self.rnn(embeddings) #\n",
    "        logits = self.projection(rnn_outputs[0]) #rnn_outpus[0] = size of vocab, as we want to predict this specific word given the chain of words\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_device = 'cuda' if torch.cuda.device_count() > 0 else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = RNNLanguageModel(options).to(current_device)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# When we may want to think about the sum loss\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'), reduction='sum')\n",
    "\n",
    "# model_parameters = [p for p in rnn_model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.Adam(model_parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          train_loader=train_loader,\n",
    "          test_loader=val_loader, \n",
    "          learning_rate=0.001,\n",
    "          num_epoch=1,\n",
    "          print_every=100):\n",
    "    # Training steps\n",
    "    start_time = time.time()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model_parameters = [p for p in rnn_model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(model_parameters,\n",
    "                                 lr=learning_rate,\n",
    "                                 weight_decay=10**(-5))\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=10**(-5))\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        for i, (data, data_len, labels) in enumerate(train_loader):\n",
    "            data, data_len, labels = data.to(device), data_len.to(device), labels.to(device)\n",
    "            outputs = model(data, data_len)\n",
    "            model.zero_grad()\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "             # report performance\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('Train set | epoch: {:3d} | {:6d}/{:6d} batches | Loss: {:6.4f}'.format(\n",
    "                    epoch, i + 1, len(train_loader), loss.item()))     \n",
    "    \n",
    "    # Evaluate after every epochh\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        truths = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, data_len, labels) in enumerate(test_loader):\n",
    "                data, data_len, labels = data.to(device), data_len.to(device), labels.to(device)\n",
    "                outputs = model(data, data_len)\n",
    "                pred = outputs.data.max(-1)[1]\n",
    "                predictions += list(pred.cpu().numpy())\n",
    "                truths += list(labels.cpu().numpy())\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum()\n",
    "                \n",
    "            acc = (100 * correct / total)\n",
    "            auc = roc_auc_score(truths, predictions)\n",
    "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "            print('Test set | Accuracy: {:6.4f} | AUC: {:4.2f} | time elapse: {:>9}'.format(\n",
    "                acc, auc, elapse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainValidModel(model, num_epochs, load_pretrained, loaders): \n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "    best_val_loss = np.inf\n",
    "    best_model_state_dict = model.state_dict()\n",
    "\n",
    "    for epoch_number in range(num_epochs):\n",
    "        avg_loss=0\n",
    "        if not load_pretrained:\n",
    "            # do train\n",
    "            model.train()\n",
    "            train_log_cache = []\n",
    "            train_loss = []\n",
    "            \n",
    "            for i, (inp, target) in enumerate(loaders['train']):\n",
    "                optimizer.zero_grad()\n",
    "                inp = inp.to(current_device)\n",
    "                target = target.to(current_device)\n",
    "                logits = model(inp)\n",
    "\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_log_cache.append(loss.item())\n",
    "                train_loss.append(loss.item())\n",
    "                \n",
    "\n",
    "                if i % 500 == 0:\n",
    "                    avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
    "                    print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
    "                    train_log_cache = []\n",
    "                    \n",
    "        train_losses.append(sum(train_loss) / len(train_loss)) \n",
    "\n",
    "        #do valid\n",
    "        valid_perplextity = []\n",
    "        valid_losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (inp, target) in enumerate(loaders['valid']):\n",
    "                inp = inp.to(current_device)\n",
    "                target = target.to(current_device)\n",
    "                logits = model(inp)\n",
    "\n",
    "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
    "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
    "            \n",
    "            if avg_val_loss <= best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_state_dict = model.state_dict()\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if load_pretrained:\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, best_val_loss, best_model_state_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
