{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Language Modeling\n",
    "\n",
    "The idea of n-gram language modeling is to calculate the probability of n-gram word w given the history h\n",
    "$$ P(w|h) = \\frac{count(w)}{count(h)}$$ In order to estimate the probability function, we can do maximum likelihood estimation.\n",
    "\n",
    "Assumption: Markov Chain property, which states that the probability of a word depends only on the previous word (rather than the history or all words precedding the interested word. \n",
    "$$ P(w | a) = \\frac{P(w,a)}{P(a)} = \\frac{count(w, a)}{\\sum{count(a)}}$$\n",
    "\n",
    "Evaluation method: Perplexity\n",
    "\n",
    "Challenge: \n",
    "\n",
    "- sparsity (i.e., count of interested word is significantly smaller than count of history or previous words).\n",
    "- dependent on training corpus.\n",
    "\n",
    "Solution to sparsity is smoothing techniques\n",
    "- simple smoothing: add 1 to the actually count \n",
    "- additive smoothing: \n",
    "$$ p = \\frac{delta + count[prefix][word]} {total[prefix] + delta*vsize} $$\n",
    "where delta is an additive term, count and total are dictionaries which store count(prefix, word) and count(prefix)\n",
    "- linear interpolation smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
    "    if not os.path.exists(filename):\n",
    "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
    "    \n",
    "    datasets = json.load(open(filename, 'r'))\n",
    "    for name in datasets:\n",
    "        datasets[name] = [x.split() for x in datasets[name]]\n",
    "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
    "    print(\"Vocab size: %d\" % (len(vocab)))\n",
    "    return datasets, vocab\n",
    "\n",
    "def perplexity(model, sequences):\n",
    "    n_total = 0\n",
    "    logp_total = 0\n",
    "    for sequence in sequences:\n",
    "        logp_total += model.sequence_logp(sequence)\n",
    "        n_total += len(sequence) + 1  \n",
    "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramAdditive(object):\n",
    "    def __init__(self, n, delta, vsize):\n",
    "        self.n = n\n",
    "        self.delta = delta\n",
    "        self.count = defaultdict(lambda: defaultdict(float))\n",
    "        self.total = defaultdict(float)\n",
    "        self.vsize = vsize\n",
    "    \n",
    "    def estimate(self, sequences):\n",
    "        for sequence in sequences:\n",
    "            padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
    "            for i in range(len(padded_sequence) - self.n+1):\n",
    "                ngram = tuple(padded_sequence[i:i+self.n])\n",
    "                prefix, word = ngram[:-1], ngram[-1]\n",
    "                self.count[prefix][word] += 1\n",
    "                self.total[prefix] += 1\n",
    "                \n",
    "    def sequence_logp(self, sequence):\n",
    "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
    "        total_logp = 0\n",
    "        for i in range(len(padded_sequence) - self.n+1):\n",
    "            ngram = tuple(padded_sequence[i:i+self.n])\n",
    "            total_logp += np.log2(self.ngram_prob(ngram))\n",
    "        return total_logp\n",
    "\n",
    "    def ngram_prob(self, ngram):\n",
    "        prefix = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        prob = ((self.delta + self.count[prefix][word]) / \n",
    "                (self.total[prefix] + self.delta*self.vsize))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 33175\n",
      "Baseline (Additive smoothing, n=2, delta=0.0005)) Train Perplexity: 90.228\n",
      "Baseline (Additive smoothing, n=2, delta=0.0005)) Valid Perplexity: 525.825\n",
      "Baseline (Additive smoothing, n=3, delta=0.0005)) Train Perplexity: 26.768\n",
      "Baseline (Additive smoothing, n=3, delta=0.0005)) Valid Perplexity: 2577.128\n",
      "Baseline (Additive smoothing, n=4, delta=0.0005)) Train Perplexity: 19.947\n",
      "Baseline (Additive smoothing, n=4, delta=0.0005)) Valid Perplexity: 9570.901\n"
     ]
    }
   ],
   "source": [
    "datasets, vocab = load_wikitext()\n",
    "\n",
    "delta = 0.0005\n",
    "for n in [2, 3, 4]:\n",
    "    lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
    "    lm.estimate(datasets['train'])\n",
    "\n",
    "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
    "    print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramInterpolation(object):\n",
    "    def __init__(self, n, lambdas, vsize):\n",
    "        self.n = n\n",
    "        self.lambdas = lambdas\n",
    "        self.vsize = vsize\n",
    "        self.count = defaultdict(lambda: defaultdict(float))\n",
    "        self.total = defaultdict(float)\n",
    "    \n",
    "    def estimate(self, sequences):\n",
    "        for n in range(1, self.n+1):\n",
    "            for sequence in sequences:\n",
    "                padded_sequence = ['<bos>']*(n-1) + sequence + ['<eos>']\n",
    "                for i in range(len(padded_sequence)-n+1):\n",
    "                    ngram = tuple(padded_sequence[i:i+n])\n",
    "                    prefix, word = ngram[:-1], ngram[-1]\n",
    "                    self.count[prefix][word] += 1\n",
    "                    self.total[prefix] += 1\n",
    "    \n",
    "    def sequence_logp(self, sequence):\n",
    "        padded_sequence = ['<bos>']*(self.n-1) + sequence + ['<eos>']\n",
    "        total_logp = 0\n",
    "        for i in range(len(padded_sequence) - self.n+1):\n",
    "            ngram = tuple(padded_sequence[i:i+self.n])\n",
    "            total_logp += np.log2(self.ngram_interp_prob(ngram))\n",
    "        return total_logp\n",
    "    \n",
    "    def ngram_prob(self, ngram):\n",
    "        prefix = ngram[:-1]\n",
    "        word = ngram[-1]\n",
    "        prob = (self.count[prefix][word] / max(self.total[prefix], 1))\n",
    "        return prob\n",
    "    \n",
    "    def ngram_interp_prob(self, sequence):\n",
    "        ngrams = []\n",
    "        for i in range(len(sequence)):\n",
    "            ngrams.append(sequence[i:])\n",
    "        probs = [self.ngram_prob(ngram) for ngram in ngrams]\n",
    "        probs.append(1.0/self.vsize)\n",
    "        interp_prob = sum([lambda_ * prob for lambda_, prob in zip(self.lambdas, probs)])\n",
    "        return interp_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results (showing $\\lambda_0,\\ldots,\\lambda_n$ values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 33175\n"
     ]
    }
   ],
   "source": [
    "datasets, vocab = load_wikitext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n = 2**: $\\lambda_2$ = 0.5, $\\lambda_1$ = 0.4, $\\lambda_0$ = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation, n=2, Train Perplexity: 127.724\n",
      "Interpolation, n=2, Valid Perplexity: 320.489\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "# in the order of lambda2, lambda1, lambda0\n",
    "lambdas = [0.5, 0.4, 0.1]\n",
    "lm = NGramInterpolation(n=n, lambdas=lambdas, vsize=len(vocab)+1)  # +1 is for <eos>\n",
    "lm.estimate(datasets['train'])\n",
    "print(\"Interpolation, n=%d, Train Perplexity: %.3f\" % (n, perplexity(lm, datasets['train'])))\n",
    "print(\"Interpolation, n=%d, Valid Perplexity: %.3f\" % (n, perplexity(lm, datasets['valid'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n=3**: $\\lambda_3$ = 0.1, $\\lambda_2$ = 0.5, $\\lambda_1$ = 0.3, $\\lambda_0$ = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation, n=3, Train Perplexity: 33.746\n",
      "Interpolation, n=3, Valid Perplexity: 283.795\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "# in the order of lambda3, lambda2, lambda1, lambda0\n",
    "lambdas = [0.1, 0.5, 0.3, 0.1]\n",
    "lm = NGramInterpolation(n=n, lambdas=lambdas, vsize=len(vocab)+1)  # +1 is for <eos>\n",
    "lm.estimate(datasets['train'])\n",
    "print(\"Interpolation, n=%d, Train Perplexity: %.3f\" % (n, perplexity(lm, datasets['train'])))\n",
    "print(\"Interpolation, n=%d, Valid Perplexity: %.3f\" % (n, perplexity(lm, datasets['valid'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n=4**: $\\lambda_4$ = 0.01, $\\lambda_3$ = 0.03, $\\lambda_2$ = 0.65, $\\lambda_1$ = 0.25, $\\lambda_0$ = 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation, n=4, Train Perplexity: 30.419\n",
      "Interpolation, n=4, Valid Perplexity: 283.463\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "# in the order of lambda4, lambda3, lambda2, lambda1, lambda0\n",
    "lambdas = [0.01, 0.03, 0.65, 0.25, 0.06]\n",
    "lm = NGramInterpolation(n=n, lambdas=lambdas, vsize=len(vocab)+1)  # +1 is for <eos>\n",
    "lm.estimate(datasets['train'])\n",
    "print(\"Interpolation, n=%d, Train Perplexity: %.3f\" % (n, perplexity(lm, datasets['train'])))\n",
    "print(\"Interpolation, n=%d, Valid Perplexity: %.3f\" % (n, perplexity(lm, datasets['valid'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation:\n",
    "\n",
    "After carefully choosing $\\lambda$ values for each n-gram interpolation model, we found that all three interpolation models outperform the additive smoothing model and performance (measured by validation perplexity) improves as n increases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
