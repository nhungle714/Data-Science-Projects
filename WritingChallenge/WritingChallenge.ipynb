{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day1.txt\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print('day' + str(i) + '.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "day1 = pd.read_csv('day1.txt', sep='delimiter', header=None, names=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(day_num):\n",
    "    day_file_name = 'day' + str(day_num) + '.txt'\n",
    "    #print(day_file_name)\n",
    "    day_csv =  pd.read_csv(day_file_name, sep='delimiter', header=None, names=['text'])\n",
    "    day_csv['day'] = day_num\n",
    "    return day_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days = [i for i in range(1, 15)]\n",
    "days.remove(6)\n",
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "total_day_csv = None\n",
    "for day_num in days:\n",
    "    day_csv = get_data(day_num)\n",
    "    #print(day_csv)\n",
    "    total_day_csv = pd.concat([total_day_csv, day_csv])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_one_dataset(one_day_dataset):\n",
    "    token_dataset = []\n",
    "    word_count = 0\n",
    "    dataset_tokenized = one_day_dataset.apply(lambda x: x.split(' '))\n",
    "    for i in range(dataset_tokenized.shape[0]):\n",
    "        tokens = [t.lower() for t in dataset_tokenized.iloc[i]]\n",
    "        word_count += len(tokens)\n",
    "        token_dataset += tokens\n",
    "    return word_count, token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, days):\n",
    "    token_datasets = []\n",
    "    all_tokens = []\n",
    "    word_count_dict = {}\n",
    "    for day_num in days:\n",
    "        print(str(day_num))\n",
    "        str_day_num = str(day_num)\n",
    "        one_day_dataset = total_day_csv.loc[total_day_csv['day']==day_num]['text']\n",
    "        word_count, token_dataset = tokenize_one_dataset(one_day_dataset)\n",
    "        word_count_dict[str_day_num] = word_count\n",
    "        token_datasets.append(token_dataset)\n",
    "        all_tokens += token_dataset\n",
    "    return token_datasets, all_tokens, word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "token_datasets, all_tokens, word_count_dict = tokenize_dataset(total_day_csv, days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 1061,\n",
       " '2': 1035,\n",
       " '3': 812,\n",
       " '4': 1010,\n",
       " '5': 928,\n",
       " '7': 1193,\n",
       " '8': 987,\n",
       " '9': 1129,\n",
       " '10': 993,\n",
       " '11': 936,\n",
       " '12': 473,\n",
       " '13': 828,\n",
       " '14': 1132}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token = build_vocab(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 1802 ; token cỡ\n",
      "Token cỡ; token id 1802\n"
     ]
    }
   ],
   "source": [
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
